Создай файл src/rss_parser.py с системой для получения новостей из RSS источников.

```python
"""
RSS Parser для торгового бота
Поддержка российских финансовых источников
"""

import asyncio
import aiohttp
import feedparser
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
import re
import html

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """Структура новостного элемента"""
    title: str
    description: str
    link: str
    published: str
    published_parsed: Optional[datetime]
    source: str
    ticker: Optional[str] = None
    sentiment: Optional[float] = None
    relevance_score: float = 0.0

class RSSParser:
    """RSS Parser с поддержкой множественных источников"""
    
    def __init__(self):
        """Инициализация RSS парсера"""
        self.session = None
        self.timeout = aiohttp.ClientTimeout(total=30)
        
        # Конфигурация RSS источников
        self.rss_sources = {
            'rbc': {
                'name': 'РБК',
                'urls': [
                    'https://rssexport.rbc.ru/rbcnews/news/20/full.rss',
                ],
                'encoding': 'utf-8'
            },
            'finam': {
                'name': 'Финам',
                'urls': [
                    'https://www.finam.ru/net/analysis/conews/rsspoint',
                ],
                'encoding': 'utf-8'
            },
        }
        
        # Маппинг тикеров к поисковым терминам
        self.ticker_keywords = {
            'SBER': ['сбербанк', 'sberbank', 'sber', 'сбер'],
            'GAZP': ['газпром', 'gazprom', 'газ'],
            'YNDX': ['яндекс', 'yandex', 'yndx'],
            'LKOH': ['лукойл', 'lukoil', 'нефть'],
            'ROSN': ['роснефть', 'rosneft', 'нефть'],
        }
        
        # Кеш для новостей
        self.news_cache = {}
        self.cache_ttl = 1800  # 30 минут

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(timeout=self.timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def get_ticker_news(self, ticker: str, hours_back: int = 24) -> List[Dict[str, Any]]:
        """
        Получение новостей по конкретному тикеру
        """
        try:
            # Проверяем кеш
            cache_key = f"{ticker}_{hours_back}"
            if self._is_cache_valid(cache_key):
                logger.info(f"Возвращаем новости {ticker} из кеша")
                return self.news_cache[cache_key]['data']
            
            if not self.session:
                self.session = aiohttp.ClientSession(timeout=self.timeout)
            
            # Получаем все новости
            all_news = await self._fetch_all_news(hours_back)
            
            # Фильтруем по тикеру
            ticker_news = self._filter_news_by_ticker(all_news, ticker)
            
            # Кешируем результат
            self.news_cache[cache_key] = {
                'data': ticker_news,
                'timestamp': datetime.now()
            }
            
            logger.info(f"Найдено {len(ticker_news)} новостей для {ticker}")
            return ticker_news
            
        except Exception as e:
            logger.error(f"Ошибка получения новостей для {ticker}: {e}")
            return []

    async def get_market_news(self, hours_back: int = 24) -> List[Dict[str, Any]]:
        """
        Получение общих рыночных новостей
        """
        try:
            cache_key = f"market_{hours_back}"
            if self._is_cache_valid(cache_key):
                return self.news_cache[cache_key]['data']
            
            if not self.session:
                self.session = aiohttp.ClientSession(timeout=self.timeout)
            
            all_news = await self._fetch_all_news(hours_back)
            
            # Фильтруем общерыночные новости
            market_news = self._filter_market_news(all_news)
            
            self.news_cache[cache_key] = {
                'data': market_news,
                'timestamp': datetime.now()
            }
            
            logger.info(f"Найдено {len(market_news)} рыночных новостей")
            return market_news
            
        except Exception as e:
            logger.error(f"Ошибка получения рыночных новостей: {e}")
            return []

    async def _fetch_all_news(self, hours_back: int) -> List[NewsItem]:
        """Получение новостей из всех источников"""
        all_news = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        # Собираем задачи для параллельного выполнения
        tasks = []
        for source_key, source_config in self.rss_sources.items():
            for url in source_config['urls']:
                task = self._fetch_source_news(url, source_config['name'], cutoff_time)
                tasks.append(task)
        
        # Выполняем все запросы параллельно
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Обрабатываем результаты
        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
            elif isinstance(result, Exception):
                logger.warning(f"Ошибка получения новостей: {result}")
        
        # Сортируем по времени публикации
        all_news.sort(key=lambda x: x.published_parsed or datetime.min, reverse=True)
        
        return all_news

    async def _fetch_source_news(self, url: str, source_name: str, cutoff_time: datetime) -> List[NewsItem]:
        """Получение новостей из одного RSS источника"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"HTTP {response.status} для {url}")
                    return []
                
                content = await response.text()
                
                # Парсим RSS
                feed = feedparser.parse(content)
                
                if not feed.entries:
                    logger.warning(f"Нет записей в RSS {url}")
                    return []
                
                news_items = []
                for entry in feed.entries:
                    try:
                        # Парсим дату публикации
                        published_dt = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            published_dt = datetime(*entry.published_parsed[:6])
                        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                            published_dt = datetime(*entry.updated_parsed[:6])
                        
                        # Пропускаем старые новости
                        if published_dt and published_dt < cutoff_time:
                            continue
                        
                        # Создаем объект новости
                        news_item = NewsItem(
                            title=self._clean_text(entry.get('title', '')),
                            description=self._clean_text(entry.get('description', '') or entry.get('summary', '')),
                            link=entry.get('link', ''),
                            published=entry.get('published', ''),
                            published_parsed=published_dt,
                            source=source_name
                        )
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        logger.warning(f"Ошибка парсинга записи RSS: {e}")
                        continue
                
                logger.info(f"Получено {len(news_items)} новостей из {source_name}")
                return news_items
                
        except asyncio.TimeoutError:
            logger.warning(f"Таймаут для {url}")
            return []
        except Exception as e:
            logger.error(f"Ошибка получения RSS {url}: {e}")
            return []

    def _filter_news_by_ticker(self, news_list: List[NewsItem], ticker: str) -> List[Dict[str, Any]]:
        """Фильтрация новостей по тикеру"""
        keywords = self.ticker_keywords.get(ticker.upper(), [ticker.lower()])
        filtered_news = []
        
        for news in news_list:
            relevance_score = self._calculate_relevance(news, keywords)
            
            if relevance_score > 0.1:  # Порог релевантности
                news_dict = asdict(news)
                news_dict['relevance_score'] = relevance_score
                news_dict['ticker'] = ticker
                filtered_news.append(news_dict)
        
        # Сортируем по релевантности
        filtered_news.sort(key=lambda x: x['relevance_score'], reverse=True)
        return filtered_news

    def _filter_market_news(self, news_list: List[NewsItem]) -> List[Dict[str, Any]]:
        """Фильтрация общерыночных новостей"""
        market_keywords = [
            'рынок', 'биржа', 'торги', 'индекс', 'ртс', 'ммвб', 'moex',
            'экономика', 'инфляция', 'цб', 'центробанк', 'ставка', 'рубль',
            'нефть', 'газ', 'доллар', 'евро', 'санкции', 'инвестиции'
        ]
        
        market_news = []
        for news in news_list:
            relevance_score = self._calculate_relevance(news, market_keywords)
            
            if relevance_score > 0.05:  # Более низкий порог для рыночных новостей
                news_dict = asdict(news)
                news_dict['relevance_score'] = relevance_score
                market_news.append(news_dict)
        
        market_news.sort(key=lambda x: x['relevance_score'], reverse=True)
        return market_news

    def _calculate_relevance(self, news: NewsItem, keywords: List[str]) -> float:
        """Расчет релевантности новости"""
        text = f"{news.title} {news.description}".lower()
        
        score = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()
            
            # Проверяем заголовок (высокий вес)
            if keyword_lower in news.title.lower():
                score += 0.5
            
            # Проверяем описание (средний вес)
            if keyword_lower in news.description.lower():
                score += 0.3
        
        return min(score, 1.0)  # Ограничиваем максимальным значением 1.0

    def _clean_text(self, text: str) -> str:
        """Очистка текста от HTML тегов и лишних символов"""
        if not text:
            return ""
        
        # Удаляем HTML теги
        text = re.sub(r'<[^>]+>', '', text)
        
        # Удаляем лишние пробелы
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Декодируем HTML entities
        text = html.unescape(text)
        
        return text

    def _is_cache_valid(self, cache_key: str) -> bool:
        """Проверка валидности кеша"""
        if cache_key not in self.news_cache:
            return False
        
        cached_time = self.news_cache[cache_key]['timestamp']
        return (datetime.now() - cached_time).seconds < self.cache_ttl

    async def close(self):
        """Закрытие сессии"""
        if self.session:
            await self.session.close()

if __name__ == "__main__":
    # Тестирование RSS парсера
    async def test_rss_parser():
        async with RSSParser() as parser:
            print("=== Тест RSS Parser ===")
            
            # Тест получения новостей по тикеру
            print("\n1. Новости по SBER:")
            sber_news = await parser.get_ticker_news("SBER", 24)
            for i, news in enumerate(sber_news[:3]):
                print(f"{i+1}. {news['title'][:60]}... (Релевантность: {news['relevance_score']:.2f})")
            
            # Тест рыночных новостей
            print("\n2. Рыночные новости:")
            market_news = await parser.get_market_news(12)
            for i, news in enumerate(market_news[:3]):
                print(f"{i+1}. {news['title'][:60]}... (Источник: {news['source']})")
    
    asyncio.run(test_rss_parser())