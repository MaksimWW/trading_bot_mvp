"""
RSS Parser –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞ - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback –ø—Ä–∏ —Å–±–æ—è—Ö Perplexity API
"""

import asyncio
import logging
import re
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import aiohttp
import feedparser

logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""

    title: str
    content: str
    url: str
    published: datetime
    source: str
    relevance_score: float = 0.0


class RSSParser:
    """RSS –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""

    def __init__(self):
        self.session = None
        self.feeds = {
            "investing": [
                "https://ru.investing.com/rss/news.rss",
            ],
            "interfax": [
                "https://www.interfax.ru/rss.asp",
            ],
            "tass": [
                "https://tass.ru/rss/v2.xml",
            ],
            "vedomosti": [
                "https://www.vedomosti.ru/rss/news",
            ],
        }

        # –¢–∏–∫–µ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö
        self.russian_tickers = {
            "SBER": ["—Å–±–µ—Ä–±–∞–Ω–∫", "sberbank", "—Å–±–µ—Ä"],
            "GAZP": ["–≥–∞–∑–ø—Ä–æ–º", "gazprom"],
            "YNDX": ["—è–Ω–¥–µ–∫—Å", "yandex"],
            "LKOH": ["–ª—É–∫–æ–π–ª", "lukoil"],
            "ROSN": ["—Ä–æ—Å–Ω–µ—Ñ—Ç—å", "rosneft"],
            "NVTK": ["–Ω–æ–≤–∞—Ç—ç–∫", "novatek"],
            "GMKN": ["–Ω–æ—Ä–Ω–∏–∫–µ–ª—å", "nornickel"],
        }

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={"User-Agent": "Trading Bot RSS Parser 1.0"},
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def fetch_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS –ª–µ–Ω—Ç—ã —Å retry –ª–æ–≥–∏–∫–æ–π"""
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

            async with self.session.get(url, headers=headers, ssl=False) as response:
                if response.status == 200:
                    content = await response.text()
                    parsed_feed = feedparser.parse(content)

                    if hasattr(parsed_feed, "entries") and len(parsed_feed.entries) > 0:
                        return parsed_feed
                    else:
                        logger.warning(f"Empty or invalid feed: {url}")
                        return None
                else:
                    logger.warning(f"Failed to fetch {url}: HTTP {response.status}")
                    return None

        except Exception as e:
            logger.error(f"Error fetching RSS feed {url}: {e}")
            return None

    def extract_ticker_relevance(self, text: str, ticker: str) -> float:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ç–∏–∫–µ—Ä–∞"""
        if not text or not ticker:
            return 0.0

        text_lower = text.lower()
        keywords = self.russian_tickers.get(ticker, [ticker.lower()])

        relevance = 0.0
        for keyword in keywords:
            mentions = len(re.findall(rf"\b{re.escape(keyword)}\b", text_lower))
            relevance += mentions * 0.3

            if keyword in text[:100].lower():
                relevance += 0.5

        return min(relevance, 1.0)

    async def get_ticker_news(self, ticker: str, hours_back: int = 24) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []

        for source, urls in self.feeds.items():
            source_news = await self._get_news_from_source(source, urls, ticker, hours_back)
            all_news.extend(source_news)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        all_news.sort(key=lambda x: x.relevance_score, reverse=True)
        return all_news[:10]

    async def _get_news_from_source(
        self, source: str, urls: List[str], ticker: str, hours_back: int
    ) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        news_items = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        for feed_url in urls:
            try:
                feed = await self.fetch_feed(feed_url)
                if not feed:
                    continue

                for entry in feed.entries[:20]:
                    news_item = self._process_feed_entry(entry, source, ticker, cutoff_time)
                    if news_item:
                        news_items.append(news_item)

            except Exception as e:
                logger.error(f"Error parsing feed {feed_url}: {e}")

        return news_items

    def _process_feed_entry(
        self, entry, source: str, ticker: str, cutoff_time: datetime
    ) -> Optional[NewsItem]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∏–∑ RSS –ª–µ–Ω—Ç—ã"""
        title = getattr(entry, "title", "")
        summary = getattr(entry, "summary", "")
        link = getattr(entry, "link", "")
        published_str = getattr(entry, "published", "")

        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
        try:
            published = feedparser._parse_date(published_str)
            if published:
                published = datetime(*published[:6])
            else:
                published = datetime.now()
        except Exception:
            published = datetime.now()

        if published < cutoff_time:
            return None

        full_text = f"{title} {summary}"
        relevance = self.extract_ticker_relevance(full_text, ticker)

        if relevance > 0.05 or any(
            keyword in full_text.lower() for keyword in ["–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "—Ä—ã–Ω–æ–∫", "—Ç–æ—Ä–≥–∏"]
        ):
            return NewsItem(
                title=title[:200],
                content=summary[:500],
                url=link,
                published=published,
                source=source,
                relevance_score=relevance,
            )

        return None

    async def test_connection(self) -> Dict[str, bool]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        results = {}

        for source, urls in self.feeds.items():
            source_available = False
            for url in urls:
                try:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            source_available = True
                            break
                except Exception:
                    continue
            results[source] = source_available

        return results


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞
async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞"""
    async with RSSParser() as parser:
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print("\nüì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:")
        status = await parser.test_connection()
        for source, available in status.items():
            icon = "‚úÖ" if available else "‚ùå"
            print(f"{icon} {source}")

        # –¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
        print("\nüì∞ –¢–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ SBER:")
        news = await parser.get_ticker_news("SBER", 48)
        print(f"–ù–∞–π–¥–µ–Ω–æ: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        for item in news[:3]:
            print(f"- [{item.source}] {item.title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {item.relevance_score:.2f})")


if __name__ == "__main__":
    asyncio.run(main())
