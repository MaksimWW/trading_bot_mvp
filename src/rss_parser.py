"""
RSS Parser –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞ - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback –ø—Ä–∏ —Å–±–æ—è—Ö Perplexity API
"""

import asyncio
import logging
import re
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import aiohttp
import feedparser

logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""

    title: str
    content: str
    url: str
    published: datetime
    source: str
    relevance_score: float = 0.0


class RSSParser:
    """RSS –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""

    def __init__(self):
        self.session = None
        self.feeds = {
            "investing": [
                "https://ru.investing.com/rss/news.rss",
            ],
            "interfax": [
                "https://www.interfax.ru/rss.asp",
            ],
            "tass": [
                "https://tass.ru/rss/v2.xml",
            ],
            "vedomosti": [
                "https://www.vedomosti.ru/rss/news",
            ],
        }

        # –¢–∏–∫–µ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö
        self.russian_tickers = {
            "SBER": ["—Å–±–µ—Ä–±–∞–Ω–∫", "sberbank", "—Å–±–µ—Ä"],
            "GAZP": ["–≥–∞–∑–ø—Ä–æ–º", "gazprom"],
            "YNDX": ["—è–Ω–¥–µ–∫—Å", "yandex"],
            "LKOH": ["–ª—É–∫–æ–π–ª", "lukoil"],
            "ROSN": ["—Ä–æ—Å–Ω–µ—Ñ—Ç—å", "rosneft"],
            "NVTK": ["–Ω–æ–≤–∞—Ç—ç–∫", "novatek"],
            "GMKN": ["–Ω–æ—Ä–Ω–∏–∫–µ–ª—å", "nornickel"],
        }

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={"User-Agent": "Trading Bot RSS Parser 1.0"},
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def check_sources_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")
        print("üì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:")

        if not self.session:
            self.session = aiohttp.ClientSession(timeout=self.timeout)

        for source_key, source_config in self.rss_sources.items():
            try:
                url = source_config['urls'][0]  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π URL
                async with self.session.get(url) as response:
                    if response.status == 200:
                        print(f"‚úÖ {source_key}")
                    else:
                        print(f"‚ùå {source_key} (HTTP {response.status})")
            except Exception as e:
                print(f"‚ùå {source_key} (–û—à–∏–±–∫–∞: {str(e)[:50]})")

    async def fetch_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS –ª–µ–Ω—Ç—ã —Å retry –ª–æ–≥–∏–∫–æ–π"""
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

            async with self.session.get(url, headers=headers, ssl=False) as response:
                if response.status == 200:
                    content = await response.text()
                    parsed_feed = feedparser.parse(content)

                    if hasattr(parsed_feed, "entries") and len(parsed_feed.entries) > 0:
                        return parsed_feed
                    else:
                        logger.warning(f"Empty or invalid feed: {url}")
                        return None
                else:
                    logger.warning(f"Failed to fetch {url}: HTTP {response.status}")
                    return None

        except Exception as e:
            logger.error(f"Error fetching RSS feed {url}: {e}")
            return None

    def extract_ticker_relevance(self, text: str, ticker: str) -> float:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ç–∏–∫–µ—Ä–∞"""
        if not text or not ticker:
            return 0.0

        text_lower = text.lower()
        keywords = self.russian_tickers.get(ticker, [ticker.lower()])

        relevance = 0.0
        for keyword in keywords:
            mentions = len(re.findall(rf"\b{re.escape(keyword)}\b", text_lower))
            relevance += mentions * 0.3

            if keyword in text[:100].lower():
                relevance += 0.5

        return min(relevance, 1.0)

    async def get_ticker_news(self, ticker: str, hours_back: int = 24) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []

        for source, urls in self.feeds.items():
            source_news = await self._get_news_from_source(source, urls, ticker, hours_back)
            all_news.extend(source_news)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        all_news.sort(key=lambda x: x.relevance_score, reverse=True)
        return all_news[:10]

    async def _get_news_from_source(
        self, source: str, urls: List[str], ticker: str, hours_back: int
    ) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        news_items = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        for feed_url in urls:
            try:
                feed = await self.fetch_feed(feed_url)
                if not feed:
                    continue

                for entry in feed.entries[:20]:
                    news_item = self._process_feed_entry(entry, source, ticker, cutoff_time)
                    if news_item:
                        news_items.append(news_item)

            except Exception as e:
                logger.error(f"Error parsing feed {feed_url}: {e}")

        return news_items

    def _process_feed_entry(
        self, entry, source: str, ticker: str, cutoff_time: datetime
    ) -> Optional[NewsItem]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∏–∑ RSS –ª–µ–Ω—Ç—ã"""
        title = getattr(entry, "title", "")
        summary = getattr(entry, "summary", "")
        link = getattr(entry, "link", "")
        published_str = getattr(entry, "published", "")

        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
        try:
            published = feedparser._parse_date(published_str)
            if published:
                published = datetime(*published[:6])
            else:
                published = datetime.now()
        except Exception:
            published = datetime.now()

        if published < cutoff_time:
            return None

        full_text = f"{title} {summary}"
        relevance = self.extract_ticker_relevance(full_text, ticker)

        if relevance > 0.05 or any(
            keyword in full_text.lower() for keyword in ["–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "—Ä—ã–Ω–æ–∫", "—Ç–æ—Ä–≥–∏"]
        ):
            return NewsItem(
                title=title[:200],
                content=summary[:500],
                url=link,
                published=published,
                source=source,
                relevance_score=relevance,
            )

        return None

    async def test_connection(self) -> Dict[str, bool]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        results = {}

        for source, urls in self.feeds.items():
            source_available = False
            for url in urls:
                try:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            source_available = True
                            break
                except Exception:
                    continue
            results[source] = source_available

        return results


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞
async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞"""
    async with RSSParser() as parser:
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print("\nüì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:")
        status = await parser.test_connection()
        for source, available in status.items():
            icon = "‚úÖ" if available else "‚ùå"
            print(f"{icon} {source}")

        # –¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
        print("\nüì∞ –¢–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ SBER:")
        news = await parser.get_ticker_news("SBER", 48)
        print(f"–ù–∞–π–¥–µ–Ω–æ: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        for item in news[:3]:
            print(f"- [{item.source}] {item.title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {item.relevance_score:.2f})")


if __name__ == "__main__":
    asyncio.run(main())
"""
RSS Parser –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
"""

import asyncio
import aiohttp
import feedparser
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
import re
import html

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    title: str
    description: str
    link: str
    published: str
    published_parsed: Optional[datetime]
    source: str
    ticker: Optional[str] = None
    sentiment: Optional[float] = None
    relevance_score: float = 0.0

class RSSParser:
    """RSS Parser —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RSS –ø–∞—Ä—Å–µ—Ä–∞"""
        self.session = None
        self.timeout = aiohttp.ClientTimeout(total=30)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.rss_sources = {
            'rbc': {
                'name': '–†–ë–ö',
                'urls': [
                    'https://rssexport.rbc.ru/rbcnews/news/20/full.rss',
                ],
                'encoding': 'utf-8'
            },
            'finam': {
                'name': '–§–∏–Ω–∞–º',
                'urls': [
                    'https://www.finam.ru/net/analysis/conews/rsspoint',
                ],
                'encoding': 'utf-8'
            },
        }

        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–∫–µ—Ä–æ–≤ –∫ –ø–æ–∏—Å–∫–æ–≤—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º
        self.ticker_keywords = {
            'SBER': ['—Å–±–µ—Ä–±–∞–Ω–∫', 'sberbank', 'sber', '—Å–±–µ—Ä'],
            'GAZP': ['–≥–∞–∑–ø—Ä–æ–º', 'gazprom', '–≥–∞–∑'],
            'YNDX': ['—è–Ω–¥–µ–∫—Å', 'yandex', 'yndx'],
            'LKOH': ['–ª—É–∫–æ–π–ª', 'lukoil', '–Ω–µ—Ñ—Ç—å'],
            'ROSN': ['—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft', '–Ω–µ—Ñ—Ç—å'],
        }

        # –ö–µ—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
        self.news_cache = {}
        self.cache_ttl = 1800  # 30 –º–∏–Ω—É—Ç

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(timeout=self.timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def check_sources_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")
        print("üì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:")

        if not self.session:
            self.session = aiohttp.ClientSession(timeout=self.timeout)

        for source_key, source_config in self.rss_sources.items():
            try:
                url = source_config['urls'][0]  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π URL
                async with self.session.get(url) as response:
                    if response.status == 200:
                        print(f"‚úÖ {source_key}")
                    else:
                        print(f"‚ùå {source_key} (HTTP {response.status})")
            except Exception as e:
                print(f"‚ùå {source_key} (–û—à–∏–±–∫–∞: {str(e)[:50]})")

    async def get_ticker_news(self, ticker: str, hours_back: int = 24) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ç–∏–∫–µ—Ä—É
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            cache_key = f"{ticker}_{hours_back}"
            if self._is_cache_valid(cache_key):
                logger.info(f"–í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ {ticker} –∏–∑ –∫–µ—à–∞")
                return self.news_cache[cache_key]['data']

            if not self.session:
                self.session = aiohttp.ClientSession(timeout=self.timeout)

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
            all_news = await self._fetch_all_news(hours_back)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É
            ticker_news = self._filter_news_by_ticker(all_news, ticker)

            # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.news_cache[cache_key] = {
                'data': ticker_news,
                'timestamp': datetime.now()
            }

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(ticker_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker}")
            return ticker_news

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker}: {e}")
            return []

    async def get_market_news(self, hours_back: int = 24) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–∏—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        try:
            cache_key = f"market_{hours_back}"
            if self._is_cache_valid(cache_key):
                return self.news_cache[cache_key]['data']

            if not self.session:
                self.session = aiohttp.ClientSession(timeout=self.timeout)

            all_news = await self._fetch_all_news(hours_back)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –æ–±—â–µ—Ä—ã–Ω–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            market_news = self._filter_market_news(all_news)

            self.news_cache[cache_key] = {
                'data': market_news,
                'timestamp': datetime.now()
            }

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(market_news)} —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return market_news

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def _fetch_all_news(self, hours_back: int) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        tasks = []
        for source_key, source_config in self.rss_sources.items():
            for url in source_config['urls']:
                task = self._fetch_source_news(url, source_config['name'], cutoff_time)
                tasks.append(task)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
            elif isinstance(result, Exception):
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {result}")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        all_news.sort(key=lambda x: x.published_parsed or datetime.min, reverse=True)

        return all_news

    async def _fetch_source_news(self, url: str, source_name: str, cutoff_time: datetime) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –æ–¥–Ω–æ–≥–æ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"HTTP {response.status} –¥–ª—è {url}")
                    return []

                content = await response.text()

                # –ü–∞—Ä—Å–∏–º RSS
                feed = feedparser.parse(content)

                if not feed.entries:
                    logger.warning(f"–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –≤ RSS {url}")
                    return []

                news_items = []
                for entry in feed.entries:
                    try:
                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                        published_dt = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            published_dt = datetime(*entry.published_parsed[:6])
                        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                            published_dt = datetime(*entry.updated_parsed[:6])

                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                        if published_dt and published_dt < cutoff_time:
                            continue

                        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏
                        news_item = NewsItem(
                            title=self._clean_text(entry.get('title', '')),
                            description=self._clean_text(entry.get('description', '') or entry.get('summary', '')),
                            link=entry.get('link', ''),
                            published=entry.get('published', ''),
                            published_parsed=published_dt,
                            source=source_name
                        )

                        news_items.append(news_item)

                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ RSS: {e}")
                        continue

                logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {source_name}")
                return news_items

        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –¥–ª—è {url}")
            return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS {url}: {e}")
            return []

    def _filter_news_by_ticker(self, news_list: List[NewsItem], ticker: str) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É"""
        keywords = self.ticker_keywords.get(ticker.upper(), [ticker.lower()])
        filtered_news = []

        for news in news_list:
            relevance_score = self._calculate_relevance(news, keywords)

            if relevance_score > 0.1:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                news_dict = asdict(news)
                news_dict['relevance_score'] = relevance_score
                news_dict['ticker'] = ticker
                filtered_news.append(news_dict)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_news.sort(key=lambda x: x['relevance_score'], reverse=True)
        return filtered_news

    def _filter_market_news(self, news_list: List[NewsItem]) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–±—â–µ—Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        market_keywords = [
            '—Ä—ã–Ω–æ–∫', '–±–∏—Ä–∂–∞', '—Ç–æ—Ä–≥–∏', '–∏–Ω–¥–µ–∫—Å', '—Ä—Ç—Å', '–º–º–≤–±', 'moex',
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '—Å—Ç–∞–≤–∫–∞', '—Ä—É–±–ª—å',
            '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '—Å–∞–Ω–∫—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'
        ]

        market_news = []
        for news in news_list:
            relevance_score = self._calculate_relevance(news, market_keywords)

            if relevance_score > 0.05:  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                news_dict = asdict(news)
                news_dict['relevance_score'] = relevance_score
                market_news.append(news_dict)

        market_news.sort(key=lambda x: x['relevance_score'], reverse=True)
        return market_news

    def _calculate_relevance(self, news: NewsItem, keywords: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏"""
        text = f"{news.title} {news.description}".lower()

        score = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–≤—ã—Å–æ–∫–∏–π –≤–µ—Å)
            if keyword_lower in news.title.lower():
                score += 0.5

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (—Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å)
            if keyword_lower in news.description.lower():
                score += 0.3

        return min(score, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 1.0

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""

        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities
        text = html.unescape(text)

        return text

    def _is_cache_valid(self, cache_key: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–µ—à–∞"""
        if cache_key not in self.news_cache:
            return False

        cached_time = self.news_cache[cache_key]['timestamp']
        return (datetime.now() - cached_time).seconds < self.cache_ttl

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞
    async def test_rss_parser():
        async with RSSParser() as parser:
            await parser.check_sources_availability()

            print("üì∞ –¢–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ SBER:")
            sber_news = await parser.get_ticker_news("SBER", 24)
            print(f"–ù–∞–π–¥–µ–Ω–æ: {len(sber_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

            for i, news in enumerate(sber_news[:2]):
                title = news['title'][:50] + "..." if len(news['title']) > 50 else news['title']
                print(f"{i+1}. {title} (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {news['relevance_score']:.2f})")

            print("\nüìà –¢–µ—Å—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:")
            market_news = await parser.get_market_news(12)
            print(f"–ù–∞–π–¥–µ–Ω–æ: {len(market_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

            for i, news in enumerate(market_news[:2]):
                title = news['title'][:50] + "..." if len(news['title']) > 50 else news['title']
                print(f"{i+1}. {title} (–ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']})")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_rss_parser())