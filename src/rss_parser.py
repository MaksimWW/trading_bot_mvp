"""
RSS Parser –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
"""

import asyncio
import html
import logging
import re
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import aiohttp
import feedparser

logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""

    title: str
    description: str
    link: str
    published: str
    published_parsed: Optional[datetime]
    source: str
    ticker: Optional[str] = None
    sentiment: Optional[float] = None
    relevance_score: float = 0.0


class RSSParser:
    """RSS Parser —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RSS –ø–∞—Ä—Å–µ—Ä–∞"""
        self.session = None
        self.timeout = aiohttp.ClientTimeout(total=30)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–±–æ—á–∏—Ö RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.rss_sources = {
            "interfax": {
                "name": "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å",
                "urls": [
                    "https://www.interfax.ru/rss.asp",
                ],
                "encoding": "utf-8",
            },
            "tass": {
                "name": "–¢–ê–°–°",
                "urls": [
                    "https://tass.ru/rss/v2.xml",
                ],
                "encoding": "utf-8",
            },
            "vedomosti": {
                "name": "–í–µ–¥–æ–º–æ—Å—Ç–∏",
                "urls": [
                    "https://www.vedomosti.ru/rss/news",
                ],
                "encoding": "utf-8",
            },
            "investing": {
                "name": "Investing.com",
                "urls": [
                    "https://ru.investing.com/rss/news.rss",
                ],
                "encoding": "utf-8",
            },
        }

        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–∫–µ—Ä–æ–≤ –∫ –ø–æ–∏—Å–∫–æ–≤—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º
        self.ticker_keywords = {
            "SBER": ["—Å–±–µ—Ä–±–∞–Ω–∫", "sberbank", "sber", "—Å–±–µ—Ä"],
            "GAZP": ["–≥–∞–∑–ø—Ä–æ–º", "gazprom", "–≥–∞–∑"],
            "YNDX": ["—è–Ω–¥–µ–∫—Å", "yandex", "yndx"],
            "LKOH": ["–ª—É–∫–æ–π–ª", "lukoil", "–Ω–µ—Ñ—Ç—å"],
            "ROSN": ["—Ä–æ—Å–Ω–µ—Ñ—Ç—å", "rosneft", "–Ω–µ—Ñ—Ç—å"],
        }

        # –ö–µ—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
        self.news_cache = {}
        self.cache_ttl = 1800  # 30 –º–∏–Ω—É—Ç

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(timeout=self.timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def check_sources_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")
        print("üì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:")

        if not self.session:
            self.session = aiohttp.ClientSession(timeout=self.timeout)

        working_sources = 0
        for source_key, source_config in self.rss_sources.items():
            try:
                url = source_config["urls"][0]  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π URL
                async with self.session.get(url) as response:
                    if response.status == 200:
                        print(f"‚úÖ {source_key}")
                        working_sources += 1
                    else:
                        print(f"‚ùå {source_key} (HTTP {response.status})")
            except Exception as e:
                print(f"‚ùå {source_key} (–û—à–∏–±–∫–∞: {str(e)[:50]})")

        print(f"üìä –†–∞–±–æ—Ç–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {working_sources}/{len(self.rss_sources)}")
        return working_sources

    async def get_ticker_news(self, ticker: str, hours_back: int = 24) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ç–∏–∫–µ—Ä—É"""
        try:
            cache_key = f"{ticker}_{hours_back}"
            if self._is_cache_valid(cache_key):
                logger.info(f"–í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ {ticker} –∏–∑ –∫–µ—à–∞")
                return self.news_cache[cache_key]["data"]

            if not self.session:
                self.session = aiohttp.ClientSession(timeout=self.timeout)

            all_news = await self._fetch_all_news(hours_back)
            ticker_news = self._filter_news_by_ticker(all_news, ticker)

            self.news_cache[cache_key] = {"data": ticker_news, "timestamp": datetime.now()}

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(ticker_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker}")
            return ticker_news

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker}: {e}")
            return []

    async def get_market_news(self, hours_back: int = 24) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–∏—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            cache_key = f"market_{hours_back}"
            if self._is_cache_valid(cache_key):
                return self.news_cache[cache_key]["data"]

            if not self.session:
                self.session = aiohttp.ClientSession(timeout=self.timeout)

            all_news = await self._fetch_all_news(hours_back)
            market_news = self._filter_market_news(all_news)

            self.news_cache[cache_key] = {"data": market_news, "timestamp": datetime.now()}

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(market_news)} —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return market_news

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def _fetch_all_news(self, hours_back: int) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        tasks = []
        for source_key, source_config in self.rss_sources.items():
            for url in source_config["urls"]:
                task = self._fetch_source_news(url, source_config["name"], cutoff_time)
                tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
            elif isinstance(result, Exception):
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {result}")

        all_news.sort(key=lambda x: x.published_parsed or datetime.min, reverse=True)
        return all_news

    async def _fetch_source_news(
        self, url: str, source_name: str, cutoff_time: datetime
    ) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –æ–¥–Ω–æ–≥–æ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            response_data = await self._get_rss_response(url)
            if not response_data:
                return []

            return self._parse_rss_entries(response_data, source_name, cutoff_time)

        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –¥–ª—è {url}")
            return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS {url}: {e}")
            return []

    async def _get_rss_response(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        async with self.session.get(url) as response:
            if response.status != 200:
                logger.warning(f"HTTP {response.status} –¥–ª—è {url}")
                return None
            return await response.text()

    def _parse_rss_entries(
        self, content: str, source_name: str, cutoff_time: datetime
    ) -> List[NewsItem]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS –∑–∞–ø–∏—Å–µ–π"""
        feed = feedparser.parse(content)

        if not feed.entries:
            logger.warning(f"–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –≤ RSS –¥–ª—è {source_name}")
            return []

        news_items = []
        for entry in feed.entries:
            try:
                news_item = self._create_news_item(entry, source_name)
                if news_item:
                    news_items.append(news_item)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ RSS: {e}")
                continue

        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {source_name}")
        return news_items[:20]

    def _create_news_item(self, entry, source_name: str) -> Optional[NewsItem]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∑–∞–ø–∏—Å–∏"""
        published_dt = None
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            published_dt = datetime(*entry.published_parsed[:6])
        elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
            published_dt = datetime(*entry.updated_parsed[:6])

        return NewsItem(
            title=self._clean_text(entry.get("title", "")),
            description=self._clean_text(entry.get("description", "") or entry.get("summary", "")),
            link=entry.get("link", ""),
            published=entry.get("published", ""),
            published_parsed=published_dt,
            source=source_name,
        )

    def _filter_news_by_ticker(
        self, news_list: List[NewsItem], ticker: str
    ) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É"""
        keywords = self.ticker_keywords.get(ticker.upper(), [ticker.lower()])
        filtered_news = []

        for news in news_list:
            relevance_score = self._calculate_relevance(news, keywords)

            if relevance_score > 0.1:
                news_dict = asdict(news)
                news_dict["relevance_score"] = relevance_score
                news_dict["ticker"] = ticker
                filtered_news.append(news_dict)

        filtered_news.sort(key=lambda x: x["relevance_score"], reverse=True)
        return filtered_news

    def _filter_market_news(self, news_list: List[NewsItem]) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–±—â–µ—Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        market_keywords = [
            "—Ä—ã–Ω–æ–∫",
            "–±–∏—Ä–∂–∞",
            "—Ç–æ—Ä–≥–∏",
            "–∏–Ω–¥–µ–∫—Å",
            "—Ä—Ç—Å",
            "–º–º–≤–±",
            "moex",
            "—ç–∫–æ–Ω–æ–º–∏–∫–∞",
            "–∏–Ω—Ñ–ª—è—Ü–∏—è",
            "—Ü–±",
            "—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫",
            "—Å—Ç–∞–≤–∫–∞",
            "—Ä—É–±–ª—å",
            "–Ω–µ—Ñ—Ç—å",
            "–≥–∞–∑",
            "–¥–æ–ª–ª–∞—Ä",
            "–µ–≤—Ä–æ",
            "—Å–∞–Ω–∫—Ü–∏–∏",
            "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
            "–∞–∫—Ü–∏–∏",
        ]

        market_news = []
        for news in news_list:
            relevance_score = self._calculate_relevance(news, market_keywords)

            if relevance_score > 0.05:
                news_dict = asdict(news)
                news_dict["relevance_score"] = relevance_score
                market_news.append(news_dict)

        market_news.sort(key=lambda x: x["relevance_score"], reverse=True)
        return market_news[:10]  # –¢–æ–ø-10 —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π

    def _calculate_relevance(self, news: NewsItem, keywords: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏"""
        score = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()

            if keyword_lower in news.title.lower():
                score += 0.5

            if keyword_lower in news.description.lower():
                score += 0.3

        return min(score, 1.0)

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""

        text = re.sub(r"<[^>]+>", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        text = html.unescape(text)

        return text

    def _is_cache_valid(self, cache_key: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–µ—à–∞"""
        if cache_key not in self.news_cache:
            return False

        cached_time = self.news_cache[cache_key]["timestamp"]
        return (datetime.now() - cached_time).seconds < self.cache_ttl

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()


if __name__ == "__main__":

    async def test_rss_parser():
        async with RSSParser() as parser:
            working_count = await parser.check_sources_availability()

            if working_count > 0:
                print("\nüì∞ –¢–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ SBER:")
                sber_news = await parser.get_ticker_news("SBER", 24)
                print(f"–ù–∞–π–¥–µ–Ω–æ: {len(sber_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

                for i, news in enumerate(sber_news[:2]):
                    title = news["title"][:50] + "..." if len(news["title"]) > 50 else news["title"]
                    print(f"{i+1}. {title} (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {news['relevance_score']:.2f})")

                print("\nüìà –¢–µ—Å—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:")
                market_news = await parser.get_market_news(12)
                print(f"–ù–∞–π–¥–µ–Ω–æ: {len(market_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

                for i, news in enumerate(market_news[:3]):
                    title = news["title"][:50] + "..." if len(news["title"]) > 50 else news["title"]
                    print(f"{i+1}. {title} (–ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']})")
            else:
                print("‚ùå –ù–µ—Ç —Ä–∞–±–æ—á–∏—Ö RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    asyncio.run(test_rss_parser())
