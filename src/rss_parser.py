"""
RSS Parser –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞ - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback –ø—Ä–∏ —Å–±–æ—è—Ö Perplexity API
"""

import asyncio
import logging
import re
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import urljoin

import aiohttp
import feedparser

logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""

    title: str
    content: str
    url: str
    published: datetime
    source: str
    relevance_score: float = 0.0


class RSSParser:
    """RSS –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""

    def __init__(self):
        self.session = None
        self.feeds = {
            "investing": [
                "https://ru.investing.com/rss/news.rss",
            ],
            "interfax": [
                "https://www.interfax.ru/rss.asp",
            ],
            "tass": [
                "https://tass.ru/rss/v2.xml",
            ],
            "vedomosti": [
                "https://www.vedomosti.ru/rss/news",
            ],
        }

        # –¢–∏–∫–µ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö
        self.russian_tickers = {
            "SBER": ["—Å–±–µ—Ä–±–∞–Ω–∫", "sberbank", "—Å–±–µ—Ä"],
            "GAZP": ["–≥–∞–∑–ø—Ä–æ–º", "gazprom"],
            "YNDX": ["—è–Ω–¥–µ–∫—Å", "yandex"],
            "LKOH": ["–ª—É–∫–æ–π–ª", "lukoil"],
            "ROSN": ["—Ä–æ—Å–Ω–µ—Ñ—Ç—å", "rosneft"],
            "NVTK": ["–Ω–æ–≤–∞—Ç—ç–∫", "novatek"],
            "GMKN": ["–Ω–æ—Ä–Ω–∏–∫–µ–ª—å", "nornickel"],
        }

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={"User-Agent": "Trading Bot RSS Parser 1.0"},
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def fetch_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS –ª–µ–Ω—Ç—ã —Å retry –ª–æ–≥–∏–∫–æ–π"""
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

            async with self.session.get(url, headers=headers, ssl=False) as response:
                if response.status == 200:
                    content = await response.text()
                    parsed_feed = feedparser.parse(content)

                    if hasattr(parsed_feed, "entries") and len(parsed_feed.entries) > 0:
                        return parsed_feed
                    else:
                        logger.warning(f"Empty or invalid feed: {url}")
                        return None
                else:
                    logger.warning(f"Failed to fetch {url}: HTTP {response.status}")
                    return None

        except Exception as e:
            logger.error(f"Error fetching RSS feed {url}: {e}")
            return None

    def extract_ticker_relevance(self, text: str, ticker: str) -> float:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ç–∏–∫–µ—Ä–∞"""
        if not text or not ticker:
            return 0.0

        text_lower = text.lower()
        keywords = self.russian_tickers.get(ticker, [ticker.lower()])

        relevance = 0.0
        for keyword in keywords:
            mentions = len(re.findall(rf"\b{re.escape(keyword)}\b", text_lower))
            relevance += mentions * 0.3

            if keyword in text[:100].lower():
                relevance += 0.5

        return min(relevance, 1.0)

    async def get_ticker_news(self, ticker: str, hours_back: int = 24) -> List[NewsItem]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        for source, urls in self.feeds.items():
            for feed_url in urls:
                try:
                    feed = await self.fetch_feed(feed_url)
                    if not feed:
                        continue

                    for entry in feed.entries[:20]:
                        title = getattr(entry, "title", "")
                        summary = getattr(entry, "summary", "")
                        link = getattr(entry, "link", "")
                        published_str = getattr(entry, "published", "")

                        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
                        try:
                            published = feedparser._parse_date(published_str)
                            if published:
                                published = datetime(*published[:6])
                            else:
                                published = datetime.now()
                        except:
                            published = datetime.now()

                        if published < cutoff_time:
                            continue

                        full_text = f"{title} {summary}"
                        relevance = self.extract_ticker_relevance(full_text, ticker)

                        if relevance > 0.05 or any(
                            keyword in full_text.lower()
                            for keyword in ["–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "—Ä—ã–Ω–æ–∫", "—Ç–æ—Ä–≥–∏"]
                        ):
                            news_item = NewsItem(
                                title=title[:200],
                                content=summary[:500],
                                url=link,
                                published=published,
                                source=source,
                                relevance_score=relevance,
                            )
                            all_news.append(news_item)

                except Exception as e:
                    logger.error(f"Error parsing feed {feed_url}: {e}")

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        all_news.sort(key=lambda x: x.relevance_score, reverse=True)
        return all_news[:10]

    async def test_connection(self) -> Dict[str, bool]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        results = {}

        for source, urls in self.feeds.items():
            source_available = False
            for url in urls:
                try:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            source_available = True
                            break
                except:
                    continue
            results[source] = source_available

        return results


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞
async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞"""
    async with RSSParser() as parser:
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print("\nüì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:")
        status = await parser.test_connection()
        for source, available in status.items():
            icon = "‚úÖ" if available else "‚ùå"
            print(f"{icon} {source}")

        # –¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
        print("\nüì∞ –¢–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ SBER:")
        news = await parser.get_ticker_news("SBER", 48)
        print(f"–ù–∞–π–¥–µ–Ω–æ: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        for item in news[:3]:
            print(f"- [{item.source}] {item.title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {item.relevance_score:.2f})")


if __name__ == "__main__":
    asyncio.run(main())
